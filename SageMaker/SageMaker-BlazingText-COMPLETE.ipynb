{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "574d80cb",
   "metadata": {},
   "source": [
    "# Clasificación de Texto usando SageMaker BlazingText\n",
    "\n",
    "La Clasificación de Texto puede ser usada para resolver varios casos de uso como análisis de sentimiento, detección de spam, predicción de hashtags, etc. Este cuaderno demuestra el uso de SageMaker BlazingText para realizar la clasificación supervisada de texto binario/multi-clase con una o varias etiquetas. BlazingText puede entrenar el modelo en más de mil millones de palabras en un par de minutos utilizando una CPU multinúcleo o una GPU, al tiempo que logra un rendimiento a la par con los algoritmos de clasificación de texto de aprendizaje profundo de última generación. BlazingText amplía el clasificador de texto fastText para aprovechar la aceleración de la GPU mediante kernels CUDA personalizados.\n",
    "\n",
    "### Setup\n",
    "\n",
    "Vamos a empezar por especificar:\n",
    "\n",
    "- El bucket de S3 y el prefijo que quieres usar para los datos de entrenamiento y modelo. Esto debería estar dentro de la misma región que la Instancia de Notebook, entrenamiento y alojamiento. Si no especificas un bucket, SageMaker SDK creará un bucket por defecto siguiendo una convención de nomenclatura predefinida en la misma región.\n",
    "\n",
    "- El rol IAM ARN utilizado para dar acceso a SageMaker a sus datos. Puede ser obtenido usando el método **get_execution_role** de sagemaker python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df3d49-20a0-43ef-9b9f-0ed9b2cea75c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\n",
    "    role\n",
    ")  # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = sess.default_bucket()  # Replace with your own bucket name if needed\n",
    "print(bucket)\n",
    "prefix = \"blazingtext/supervised\"  # Replace with the prefix under which you want to store the data if needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d53a866",
   "metadata": {},
   "source": [
    "### Preparación de los datos\n",
    "\n",
    "Ahora descargaremos un conjunto de datos de la web con el que queremos entrenar el modelo de clasificación de texto. BlazingText espera un único archivo de texto preprocesado con tokens separados por espacios y cada línea del archivo debe contener una única frase y la(s) etiqueta(s) correspondiente(s) precedida(s) por \"\\__label\\__\".\n",
    "\n",
    "En este ejemplo, vamos a entrenar el modelo de clasificación de texto en el [DBPedia Ontology Dataset](https://wiki.dbpedia.org/services-resources/dbpedia-data-set-2014#2) tal y como hicieron [Zhang et al](https://arxiv.org/pdf/1509.01626.pdf). El conjunto de datos ontológicos DBpedia se construye eligiendo 14 clases no superpuestas de DBpedia 2014. Tiene 560.000 muestras de entrenamiento y 70.000 muestras de prueba. Los campos que utilizamos para este conjunto de datos contienen el título y el resumen de cada artículo de Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9fadf0-38e6-4343-ac6c-d57e23b95b47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/saurabh3949/Text-Classification-Datasets/raw/master/dbpedia_csv.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc65e9-9b53-4058-ab1f-7c03a668cbc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -xzvf dbpedia_csv.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b49f8e-5560-4e85-89a5-28ced4637b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head dbpedia_csv/train.csv -n 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b510485",
   "metadata": {},
   "source": [
    "Como puede verse en el resultado anterior, el CSV tiene 3 campos: índice de etiqueta, título y resumen. En primer lugar, vamos a crear un índice de etiqueta para asignar el nombre de la etiqueta y luego proceder a preprocesar el conjunto de datos para la ingestión por BlazingText.\n",
    "\n",
    "El siguiente código crea la correspondencia entre los índices enteros y la etiqueta de la clase, que se utilizará posteriormente para recuperar el nombre real de la clase durante la inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0ba1f3-6077-46c0-affe-1453865c32a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_to_label = {}\n",
    "with open(\"dbpedia_csv/classes.txt\") as f:\n",
    "    for i, label in enumerate(f.readlines()):\n",
    "        index_to_label[str(i + 1)] = label.strip()\n",
    "index_to_label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd77180e",
   "metadata": {},
   "source": [
    "### Preprocesamiento de datos\n",
    "Necesitamos preprocesar los datos de entrenamiento en formato de **texto tokenizado separado por espacios** que pueda ser consumido por el algoritmo `BlazingText`. Además, como se ha mencionado anteriormente, las etiquetas de clase deben ir precedidas de `__label__` y deben aparecer en la misma línea que la frase original. Utilizaremos la biblioteca `nltk` para tokenizar las frases de entrada del conjunto de datos DBPedia. \n",
    "\n",
    "Descargar el tokenizador nltk y otras bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea00094-bf13-4e59-afd7-2d21691cd5e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d34db-94a1-4fa6-bb67-8710f997c5f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    label = \"__label__\" + index_to_label[row[0]]  # Prefix the index-ed label with __label__\n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(row[1].lower()))\n",
    "    cur_row.extend(nltk.word_tokenize(row[2].lower()))\n",
    "    return cur_row"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1a337ab",
   "metadata": {},
   "source": [
    "* La `transform_instance` se aplicará a cada instancia de datos en paralelo utilizando el módulo de multiprocesamiento de python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb95722-0cac-4add-a1f7-a5b39623e3cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(input_file, output_file, keep=1):\n",
    "    all_rows = []\n",
    "    with open(input_file, \"r\") as csvinfile:\n",
    "        csv_reader = csv.reader(csvinfile, delimiter=\",\")\n",
    "        for row in csv_reader:\n",
    "            all_rows.append(row)\n",
    "    shuffle(all_rows)\n",
    "    all_rows = all_rows[: int(keep * len(all_rows))]\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_instance, all_rows)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    with open(output_file, \"w\") as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=\" \", lineterminator=\"\\n\")\n",
    "        csv_writer.writerows(transformed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20858c4-a330-471b-b537-4891c4d30231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Preparing the training dataset\n",
    "\n",
    "# Since preprocessing the whole dataset might take a couple of mintutes,\n",
    "# we keep 20% of the training dataset for this demo.\n",
    "# Set keep to 1 if you want to use the complete dataset\n",
    "preprocess(\"dbpedia_csv/train.csv\", \"dbpedia.train\", keep=0.8)\n",
    "\n",
    "# Preparing the validation dataset\n",
    "preprocess(\"dbpedia_csv/test.csv\", \"dbpedia.validation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89e3aa46",
   "metadata": {},
   "source": [
    "La celda de preprocesamiento de datos puede tardar un minuto en ejecutarse. Después de que el preprocesamiento de datos esté completo, necesitamos subirlo a S3 para que pueda ser consumido por SageMaker para ejecutar trabajos de entrenamiento. Usaremos Python SDK para subir estos dos archivos al bucket y a la ubicación del prefijo que hemos establecido anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266bcd43-25f9-438c-8b7f-069990657cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + \"/train\"\n",
    "validation_channel = prefix + \"/validation\"\n",
    "\n",
    "sess.upload_data(path=\"dbpedia.train\", bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path=\"dbpedia.validation\", bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = \"s3://{}/{}\".format(bucket, train_channel)\n",
    "s3_validation_data = \"s3://{}/{}\".format(bucket, validation_channel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8de46c6c",
   "metadata": {},
   "source": [
    "A continuación tenemos que configurar una ubicación de salida en S3, donde se volcará el artefacto modelo. Estos artefactos son también la salida del trabajo de traning del algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f267007b-0e34-47ca-9be6-d764ea18f94e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_output_location = \"s3://{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "803aedd2",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "Ahora que hemos terminado con toda la configuración necesaria, estamos listos para entrenar nuestro detector de objetos. Para empezar, vamos a crear un objeto ``sageMaker.estimator.Estimator``. Este estimador lanzará el trabajo de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c5c2c-313e-44b8-aa3b-4881c0c8079a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ad7eff-aa56-42c2-b6af-5d77cefa9eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    region=region_name, \n",
    "    framework=\"blazingtext\", \n",
    "    version=\"1\"\n",
    ")\n",
    "print(\"Using SageMaker BlazingText container: {} ({})\".format(container, region_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aecc8bc4",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo BlazingText para la clasificación supervisada de textos\n",
    "\n",
    "De forma similar a la implementación original de [Word2Vec](https://arxiv.org/pdf/1301.3781.pdf), SageMaker BlazingText proporciona una implementación eficiente de las arquitecturas de bolsa continua de palabras (CBOW) y skip-gram utilizando Muestreo Negativo, en CPUs y adicionalmente en GPU[s]. La implementación en GPU utiliza núcleos CUDA altamente optimizados. Para obtener más información, consulte [*BlazingText: Escalado y aceleración de Word2Vec utilizando múltiples GPU*](https://dl.acm.org/citation.cfm?doid=3146347.3146354).\n",
    "\n",
    "Además de skip-gram y CBOW, SageMaker BlazingText también soporta el modo \"Batch Skipgram\", que utiliza eficientes operaciones mini-batch y matriz-matriz ([BLAS Level 3 routines](https://software.intel.com/en-us/mkl-developer-reference-fortran-blas-level-3-routines)). Este modo permite el entrenamiento distribuido de word2vec a través de múltiples nodos CPU, permitiendo un escalado casi lineal del cómputo word2vec para procesar cientos de millones de palabras por segundo. Para más información, consulte [*Paralelización de Word2Vec en memoria compartida y distribuida*](https://arxiv.org/pdf/1604.04661.pdf).\n",
    "\n",
    "BlazingText también admite un modo *supervisado* para la clasificación de textos. Amplía el clasificador de texto FastText para aprovechar la aceleración de la GPU mediante kernels CUDA personalizados. El modelo puede entrenarse con más de mil millones de palabras en un par de minutos utilizando una CPU multinúcleo o una GPU, al tiempo que alcanza un rendimiento equiparable al de los algoritmos de clasificación de texto de aprendizaje profundo más avanzados. Para más información, consulte la [documentación del algoritmo](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "268aa5d3",
   "metadata": {},
   "source": [
    "En resumen, BlazingText admite los siguientes modos en diferentes tipos de instancias:\n",
    "\n",
    "|          Modes         \t| cbow (supports subwords training) \t| skipgram (supports subwords training) \t| batch_skipgram \t| supervised |\n",
    "|:----------------------:\t|:----:\t|:--------:\t|:--------------:\t| :--------------:\t|\n",
    "|   Single CPU instance  \t|   ✔  \t|     ✔    \t|        ✔       \t|  ✔  |\n",
    "|   Single GPU instance  \t|   ✔  \t|     ✔    \t|                \t|  ✔ (Instance with 1 GPU only)  |\n",
    "| Multiple CPU instances \t|      \t|          \t|        ✔       \t|     | |\n",
    "\n",
    "Ahora, definamos el `Estimator` de SageMaker con configuraciones de recursos e hiperparámetros para entrenar la Clasificación de Texto en el conjunto de datos *DBPedia*, usando el modo \"supervisado\" en una instancia `c4.4xlarge`.\n",
    "\n",
    "Consulte [BlazingText Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html) en la documentación de Amazon SageMaker para obtener la lista completa de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221b7e9-b5ed-437f-85a3-d1a3bc6c2e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.4xlarge\",\n",
    "    volume_size=30,\n",
    "    max_run=360000,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    hyperparameters={\n",
    "        \"mode\": \"supervised\",\n",
    "        \"epochs\": 1,\n",
    "        \"min_count\": 2,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"vector_dim\": 10,\n",
    "        \"early_stopping\": True,\n",
    "        \"patience\": 4,\n",
    "        \"min_epochs\": 5,\n",
    "        \"word_ngrams\": 2,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18e3e748",
   "metadata": {},
   "source": [
    "Ahora que los hiper-parámetros están configurados, preparemos el *handshake* entre nuestros canales de datos y el algoritmo. Para ello, tenemos que crear los objetos `sagemaker.session.s3_input` de nuestros canales de datos. Estos objetos se colocan en un diccionario simple, que el algoritmo consume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb7d2e-2205-464f-820f-cdfd2532b0bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_train_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_validation_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8893aa57",
   "metadata": {},
   "source": [
    "Tenemos nuestro objeto `Estimador`, hemos configurado los hiperparámetros para este objeto y tenemos nuestros canales de datos enlazados con el algoritmo. Lo único que queda por hacer es entrenar el algoritmo. El siguiente comando entrenará el algoritmo. El entrenamiento del algoritmo consta de varios pasos. En primer lugar, se aprovisiona la instancia que solicitamos al crear las clases `Estimator` y se configura con las librerías apropiadas. A continuación, los datos de nuestros canales se descargan en la instancia. Una vez hecho esto, comienza el trabajo de entrenamiento. El aprovisionamiento y la descarga de datos llevarán algún tiempo, dependiendo del tamaño de los datos. Por lo tanto, pueden pasar unos minutos antes de que empecemos a obtener los registros de entrenamiento de nuestros trabajos de entrenamiento. Los registros de datos también imprimirán la Precisión en los datos de validación para cada época después de que el trabajo de entrenamiento haya ejecutado `min_epochs`. Esta métrica es un indicador de la calidad del algoritmo. \n",
    "\n",
    "Una vez finalizado el trabajo, se imprimirá el mensaje \"Trabajo completado\". El modelo entrenado se puede encontrar en el bucket de S3 que se configuró como `output_path` en el estimador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153acdaf-25e6-4d27-876c-6728d33a2390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d3a322a",
   "metadata": {},
   "source": [
    "### Alojamiento / Inferencia\n",
    "Una vez realizado el entrenamiento, podemos desplegar el modelo entrenado como un endpoint alojado en tiempo real de Amazon SageMaker. Esto nos permitirá realizar predicciones (o inferencias) a partir del modelo. Tenga en cuenta que no tenemos que alojar en el mismo tipo de instancia que utilizamos para entrenar. Dado que los puntos finales de instancia estarán en funcionamiento durante mucho tiempo, es aconsejable elegir una instancia más barata para la inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e4ab2-5dad-44a2-bc21-5d797ddbbefd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "text_classifier = bt_model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m4.xlarge\", serializer=JSONSerializer()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5277dfca",
   "metadata": {},
   "source": [
    "#### Utilizar el formato JSON para la inferencia\n",
    "\n",
    "BlazingText soporta `application/json` como tipo de contenido para la inferencia. La carga útil debe contener una lista de sentencias con la clave \"**instances**\" mientras se pasa al endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70de48e-ae28-4062-a73c-b9ba9be2098c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Convair was an american aircraft manufacturing company which later expanded into rockets and spacecraft.\",\n",
    "    \"Berwick secondary college is situated in the outer melbourne metropolitan suburb of berwick .\",\n",
    "]\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [\" \".join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "payload = {\"instances\": tokenized_sentences}\n",
    "\n",
    "response = text_classifier.predict(payload)\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3128eb8",
   "metadata": {},
   "source": [
    "Por defecto, el modelo sólo devuelve una predicción, la de mayor probabilidad. Para recuperar las k predicciones más altas, puede establecer `k` en la configuración como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dcee78-7788-460a-8e98-2b7938b0c7ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"instances\": tokenized_sentences, \"configuration\": {\"k\": 2}}\n",
    "\n",
    "response = text_classifier.predict(payload)\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad60271b",
   "metadata": {},
   "source": [
    "### Detener / Cerrar el Endpoint (Opcional)\n",
    "Por último, deberíamos eliminar el endpoint antes de cerrar el bloc de notas si no necesitamos mantener el endpoint en ejecución para servir predicciones en tiempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#sess.delete_endpoint(text_classifier.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac9033-3bed-4b23-9b18-f2e4caad0e66",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "761620e6",
   "metadata": {},
   "source": [
    "## Crear un punto final de inferencia asíncrono\n",
    "\n",
    "Cree un endpoint asíncrono de la misma forma que crearía un endpoint utilizando los servicios de alojamiento de SageMaker:\n",
    "\n",
    "* Cree un modelo en SageMaker con CreateModel.\n",
    "* Cree una configuración de endpoint con CreateEndpointConfig.\n",
    "* Cree un endpoint HTTPS con CreateEndpoint.\n",
    "\n",
    "Para crear un endpoint, primero se crea un modelo con CreateModel, donde se apunta al artefacto del modelo y a una ruta de registro Docker (Image). A continuación, cree una configuración mediante CreateEndpointConfig, donde especifique uno o más modelos creados mediante la API CreateModel para desplegar y los recursos que desea que SageMaker aprovisione. Cree su punto final con CreateEndpoint utilizando la configuración del punto final especificada en la solicitud. Puede actualizar un endpoint asíncrono con la API UpdateEndpoint. Envíe y reciba solicitudes de inferencia del modelo alojado en el endpoint con InvokeEndpointAsync. Puede eliminar sus puntos finales con la API DeleteEndpoint.\n",
    "\n",
    "Para una lista completa de las Imágenes SageMaker disponibles, vea [Imágenes de Contenedores de Aprendizaje Profundo Disponibles](https://github.com/aws/deep-learning-containers/blob/master/available_images.md). Vea [Use su propio código de inferencia](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-main.html) para información sobre cómo crear su imagen Docker.\n",
    "\n",
    "### Crear un modelo\n",
    "El siguiente ejemplo muestra cómo crear un modelo utilizando el SDK de AWS para Python (Boto3). Las primeras líneas definen\n",
    "\n",
    "* `sagemaker_client`: Un objeto cliente de bajo nivel de SageMaker que facilita el envío y recepción de solicitudes a los servicios de AWS.\n",
    "* `sagemaker_role`: Una variable de cadena con el nombre de recurso de Amazon (ARN) del rol IAM de SageMaker.\n",
    "* `aws_region`: Una variable de cadena con el nombre de tu región de AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e90674-4af0-4f56-aed6-96abec204c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Specify your AWS Region\n",
    "aws_region='us-east-1'\n",
    "\n",
    "# Create a low-level SageMaker service client.\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
    "\n",
    "# Role to give SageMaker permission to access AWS services.\n",
    "sagemaker_role= \"arn:aws:iam::763864518324:role/Lab-AmazonSageMaker-ExecutionRole\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "994813a3",
   "metadata": {},
   "source": [
    "A continuación, especifique la ubicación del modelo preentrenado almacenado en Amazon S3. En este ejemplo, utilizamos un modelo XGBoost preentrenado denominado demo-xgboost-model.tar.gz. La URI completa de Amazon S3 se almacena en una variable de cadena model_url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ad057a-90fd-42bf-b9ce-504d3ce571a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a variable w/ the model S3 URI\n",
    "s3_bucket = 'sagemaker-us-east-1-763864518324' # Provide the name of your S3 bucket\n",
    "bucket_prefix='blazingtext'\n",
    "model_s3_key = f\"{bucket_prefix}/supervised/output/blazingtext-2023-04-09-14-04-53-267/output/model.tar.gz\"\n",
    "\n",
    "#Specify S3 bucket w/ model\n",
    "model_url = f\"s3://{s3_bucket}/{model_s3_key}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9d3f40d",
   "metadata": {},
   "source": [
    "Especifique un contenedor primario. Para el contenedor primario, se especifica la imagen Docker que contiene el código de inferencia, los artefactos (del entrenamiento anterior) y un mapa de entorno personalizado que el código de inferencia utiliza cuando se despliega el modelo para las predicciones.\n",
    "\n",
    "En este ejemplo, especificamos una imagen de contenedor de algoritmo integrado XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c922ba13-b34e-49cd-b550-cbabed65d3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "# Specify an AWS container image. \n",
    "container = image_uris.retrieve(region=aws_region, framework='xgboost', version='0.90-1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19a7a108",
   "metadata": {},
   "source": [
    "Cree un modelo en Amazon SageMaker con CreateModel. Especifique lo siguiente\n",
    "\n",
    "* `ModelName`: Un nombre para su modelo (en este ejemplo se almacena como una variable de cadena llamada model_name).\n",
    "\n",
    "* `ExecutionRoleArn`: El nombre de recurso de Amazon (ARN) del rol de IAM que Amazon SageMaker puede asumir para obtener acceso a los artefactos del modelo y las imágenes de Docker para el despliegue en instancias informáticas de ML o para trabajos de transformación por lotes.\n",
    "\n",
    "* `PrimaryContainer`: La ubicación de la imagen Docker principal que contiene el código de inferencia, los artefactos asociados y los mapas de entorno personalizados que utiliza el código de inferencia cuando se despliega el modelo para las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbb0dd6-0c1a-442e-95bc-eb6bbd8bb541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'my-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22f885-b361-40b6-9956-d921893c97b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create model\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = sagemaker_role,\n",
    "    PrimaryContainer = {\n",
    "        'Image': container,\n",
    "        'ModelDataUrl': model_url,\n",
    "    })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9a9d77d",
   "metadata": {},
   "source": [
    "Si está utilizando un contenedor proporcionado por SageMaker, puede aumentar el tiempo de espera del servidor de modelos y los tamaños de la carga útil de los valores predeterminados a los máximos admitidos por el marco estableciendo variables de entorno en este paso. Es posible que no pueda aprovechar los tamaños máximos de tiempo de espera y carga útil que admite Asynchronous Inference si no establece explícitamente estas variables. El siguiente ejemplo muestra cómo puede establecer las variables de entorno para un contenedor PyTorch Inference basado en TorchServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_model_response = sagemaker_client.create_model(\r\n",
       "    ModelName = model_name,\r\n",
       "    ExecutionRoleArn = sagemaker_role,\r\n",
       "    PrimaryContainer = {\r\n",
       "        'Image': container,\r\n",
       "        'ModelDataUrl': model_url,\r\n",
       "        'Environment': {\r\n",
       "            'TS_MAX_REQUEST_SIZE': '100000000',\r\n",
       "            'TS_MAX_RESPONSE_SIZE': '100000000',\r\n",
       "            'TS_DEFAULT_RESPONSE_TIMEOUT': '1000'\r\n",
       "        },\r\n",
       "    })"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_model_response_torch = sagemaker_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = sagemaker_role,\n",
    "    PrimaryContainer = {\n",
    "        'Image': container,\n",
    "        'ModelDataUrl': model_url,\n",
    "        'Environment': {\n",
    "            'TS_MAX_REQUEST_SIZE': '100000000',\n",
    "            'TS_MAX_RESPONSE_SIZE': '100000000',\n",
    "            'TS_DEFAULT_RESPONSE_TIMEOUT': '1000'\n",
    "        },\n",
    "    })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9d71f81",
   "metadata": {},
   "source": [
    "Cuando termines de crear tu endpoint, deberías comprobar que has configurado correctamente las variables de entorno imprimiéndolas desde tu script inference.py. La siguiente tabla enumera las variables de entorno de varios frameworks que puedes configurar para cambiar los valores predeterminados.\n",
    "\n",
    "<table id=\"w1172aac25c35c11b9c11c33\"><thead>\n",
    "                        <tr>\n",
    "                            <th>Framework</th>\n",
    "                            <th>Environment variables</th>\n",
    "                        </tr>\n",
    "                    </thead>\n",
    "                        <tbody><tr>\n",
    "                            <td>\n",
    "                                <p>PyTorch 1.8 (based on TorchServe)</p>\n",
    "                            </td>\n",
    "                            <td>\n",
    "                                <p>'TS_MAX_REQUEST_SIZE': '100000000'</p>\n",
    "                                <p>'TS_MAX_RESPONSE_SIZE': '100000000'</p>\n",
    "                                <p>'TS_DEFAULT_RESPONSE_TIMEOUT': '1000'</p>\n",
    "                            </td>\n",
    "                        </tr>\n",
    "                        <tr>\n",
    "                            <td>\n",
    "                                <p>PyTorch 1.4 (based on MMS)</p>\n",
    "                            </td>\n",
    "                            <td>\n",
    "                                <p>'MMS_MAX_REQUEST_SIZE': '1000000000'</p>\n",
    "                                <p>'MMS_MAX_RESPONSE_SIZE': '1000000000'</p>\n",
    "                                <p>'MMS_DEFAULT_RESPONSE_TIMEOUT': '900'</p>\n",
    "                            </td>\n",
    "                        </tr>\n",
    "                        <tr>\n",
    "                            <td>\n",
    "                                <p>HuggingFace Inference Container (based on MMS)</p>\n",
    "                            </td>\n",
    "                            <td>\n",
    "                                <p>'MMS_MAX_REQUEST_SIZE': '2000000000'</p>\n",
    "                                <p>'MMS_MAX_RESPONSE_SIZE': '2000000000'</p>\n",
    "                                <p>'MMS_DEFAULT_RESPONSE_TIMEOUT': '900'</p>\n",
    "                            </td>\n",
    "                        </tr>\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d18018-5a69-4719-98e3-6fc8124822de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = client.list_models(\n",
    "    SortBy='Name',\n",
    "    SortOrder='Ascending',\n",
    "    CreationTimeAfter=time_after\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d84842e",
   "metadata": {},
   "source": [
    "### Crear una configuración de punto final\n",
    "\n",
    "Una vez que tenga un modelo, cree una configuración de punto final con CreateEndpointConfig. Los servicios de hospedaje de Amazon SageMaker utilizan esta configuración para desplegar modelos. En la configuración, identifique uno o varios modelos, creados con CreateModel, para implementar los recursos que desea que aprovisione Amazon SageMaker. Especifique el objeto AsyncInferenceConfig y proporcione una ubicación de salida de Amazon S3 para OutputConfig. Opcionalmente, puede especificar temas de Amazon SNS en los que enviar notificaciones sobre los resultados de las predicciones. Para obtener más información sobre los temas de Amazon SNS, consulte Configuración de Amazon SNS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83485c52-afe6-462b-8a50-6d55cd768bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23d1d18-1df1-4869-ac0a-b0736c47c773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an endpoint config name. Here we create one based on the date  \n",
    "# so it we can search endpoints based on creation time.\n",
    "endpoint_config_name = f\"BlazingTextEndpointConfig-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "\n",
    "create_endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name, # You will specify this name in a CreateEndpoint request.\n",
    "    # List of ProductionVariant objects, one for each model that you want to host at this endpoint.\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\", # The name of the production variant.\n",
    "            \"ModelName\": model_name, \n",
    "            \"InstanceType\": \"ml.m5.xlarge\", # Specify the compute instance type.\n",
    "            \"InitialInstanceCount\": 1 # Number of instances to launch initially.\n",
    "        }\n",
    "    ],\n",
    "    AsyncInferenceConfig={\n",
    "        \"OutputConfig\": {\n",
    "            # Location to upload response outputs when no location is provided in the request.\n",
    "            \"S3OutputPath\": f\"s3://{s3_bucket}/{bucket_prefix}/output\",\n",
    "            # (Optional) specify Amazon SNS topics\n",
    "            #\"NotificationConfig\": {\n",
    "            #    \"SuccessTopic\": \"arn:aws:sns:aws-region:account-id:topic-name\",\n",
    "            #   \"ErrorTopic\": \"arn:aws:sns:aws-region:account-id:topic-name\",\n",
    "            },\n",
    "        \"ClientConfig\": {\n",
    "            # (Optional) Specify the max number of inflight invocations per instance\n",
    "            # If no value is provided, Amazon SageMaker will choose an optimal value for you\n",
    "            \"MaxConcurrentInvocationsPerInstance\": 4\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Created EndpointConfig: {create_endpoint_config_response['EndpointConfigArn']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b9a0405",
   "metadata": {},
   "source": [
    "En el ejemplo anterior, se especifican las siguientes claves para OutputConfig para el campo AsyncInferenceConfig:\n",
    "\n",
    "* `S3OutputPath`: Ubicación para cargar las salidas de respuesta cuando no se proporciona ninguna ubicación en la solicitud.\n",
    "* `NotificationConfig`: (Opcional) Temas SNS que le envían notificaciones cuando una solicitud de inferencia tiene éxito (SuccessTopic) o si falla (ErrorTopic).\n",
    "\n",
    "También puede especificar el siguiente argumento opcional para ClientConfig en el campo AsyncInferenceConfig:\n",
    "\n",
    "* `MaxConcurrentInvocationsPerInstance`: (Opcional) El número máximo de solicitudes concurrentes enviadas por el cliente SageMaker al contenedor del modelo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c44e8da7",
   "metadata": {},
   "source": [
    "### Crear punto final\n",
    "\n",
    "Una vez que tenga su modelo y configuración de punto de enlace, utilice la API CreateEndpoint para crear su punto de enlace. El nombre del punto de enlace debe ser único dentro de una región de AWS en su cuenta de AWS.\n",
    "\n",
    "A continuación se crea un endpoint utilizando la configuración del endpoint especificada en la solicitud. Amazon SageMaker utiliza el punto de enlace para aprovisionar recursos e implementar modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d8e1e-d3a1-44b8-b68d-c09256581308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The name of the endpoint.The name must be unique within an AWS Region in your AWS account.\n",
    "endpoint_name = f\"BlazingTextEndpoint-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "\n",
    "create_endpoint_response = sagemaker_client.create_endpoint(\n",
    "                                            EndpointName=endpoint_name, \n",
    "                                            EndpointConfigName=endpoint_config_name) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9475d8a9-1e04-4ae4-84f3-edae364b69ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the endpoint status\n",
    "response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print(f\"Endpoint status is {status}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6f9e528",
   "metadata": {},
   "source": [
    "Cuando llama a la API `CreateEndpoint`, Amazon SageMaker Asynchronous Inference envía una notificación de prueba para comprobar que ha configurado un tema de Amazon SNS. Amazon SageMaker Asynchronous Inference también envía notificaciones de prueba después de las llamadas a `UpdateEndpoint` y `UpdateEndpointWeightsAndCapacities`. Esto permite a SageMaker comprobar que dispone de los permisos necesarios. La notificación puede ser simplemente ignorada. La notificación de prueba tiene la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495298da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a low-level client representing Amazon SageMaker Runtime\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\", region_name=aws_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input payload for the API\n",
    "input_data = {'key': 'value'}\n",
    "payload = json.dumps(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the endpoint asynchronously\n",
    "response = sm_runtime.invoke_endpoint_async(EndpointName=endpoint_name, ContentType='application/json', Body=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69685e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the inference job ID from the response\n",
    "job_id = response['InferenceId']\n",
    "print(f\"Inference job ID is {job_id}\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   }
  ],
  "instance_type": "ml.m5d.4xlarge",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
