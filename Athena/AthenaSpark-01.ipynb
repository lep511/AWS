{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "bucket = \"sdl-immersion-day-644711630487\"\n",
    "path = f\"s3://{bucket}/raw/\"\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(bucket)\n",
    "prefix = 'raw'\n",
    "list_files = []\n",
    "for obj in my_bucket.objects.filter(Prefix=prefix):\n",
    "    list_files.append(f\"s3://{bucket}/\" + obj.key)\n",
    "print(\"Total files: \", len(list_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(list_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretty print a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = df.sample(15 / df.count())\n",
    "var1 = sample_data.collect()\n",
    "%table var1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, explode\n",
    "\n",
    "df.select(\"color\", \"product\")\\\n",
    "    .where(col(\"product\") == \"Chips\")\\\n",
    "    .groupBy(\"product\", \"color\")\\\n",
    "    .agg(count(\"color\").alias(\"ones\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can filter our data based on multiple conditions.\n",
    "df.filter((df.campaign=='BlackFriday')).select('productName','product', 'department', 'price','campaign').limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"s3://{bucket}/results/csv/\"\n",
    "df.coalesce(1).write.mode('overwrite').csv(path)\n",
    "print(f\"Write to {path} complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "sql_consult = \"\"\"\n",
    "    SELECT count(product) as total, product, department\n",
    "    FROM products\n",
    "    WHERE department == 'Outdoors'\n",
    "    GROUP BY department, product\n",
    "\"\"\"\n",
    "sqlDF = spark.sql(sql_consult)\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new database: spark_demo_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create database if not exists spark_demo_database\")\n",
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new table \n",
    "\n",
    "Use the spark_demo_database and the taxi1_df dataframe created earlier to cerate a new table: select_taxi_table. The table is also saved to S3\n",
    "Note: you will need to update IAM to have write permissions to S3://644711630487-us-east-1-athena-results-bucket-8usz9um3wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"use spark_demo_database\")\n",
    "sqlDF.write.mode(\"overwrite\").format(\"csv\").option(\"path\",path).saveAsTable(\"outdoors_products\")\n",
    "print(f\"Create new table from {path} complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the new table in the spark_demo_database\n",
    "Note: you should be able to go to Glue console and see the new database and table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the new table data\n",
    "spark.sql(\"select * from select_taxi_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.clf()\n",
    "df=sqlDF.toPandas()\n",
    "df.sort_values('total',inplace=True)\n",
    "plt.barh(df['product'], df['total'])\n",
    "%matplot plt"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
