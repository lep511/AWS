{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "from spdynamodb import DynamoTable\n",
    "import json\n",
    "from decimal import Decimal\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully!\n"
     ]
    }
   ],
   "source": [
    "dt=DynamoTable()\n",
    "try:\n",
    "    dt.select_table('LogFileScan')\n",
    "    print(dt)\n",
    "except:\n",
    "    dt.create_table(\n",
    "        table_name='LogFileScan',\n",
    "        partition_key='PK',\n",
    "        partition_key_type='S',\n",
    "        rcu=5000,\n",
    "        wcu=5000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global secondary index is being created, this may take a few minutes...\n",
      "Global secondary index created. Time elapsed: 6.53 minute\n"
     ]
    }
   ],
   "source": [
    "dt.create_global_secondary_index(\n",
    "    att_name=\"GSI_1_PK\",\n",
    "    att_type=\"S\",\n",
    "    sort_index=\"GSI_1_SK\",\n",
    "    sort_type=\"S\",\n",
    "    proj_type=\"KEYS_ONLY\",\n",
    "    i_rcu=3000,\n",
    "    i_wcu=5000,\n",
    "    i_name=\"GSI_1\"\n",
    ")\n",
    "\n",
    "status = dt.check_status_gsi()\n",
    "if status == 'CREATING':\n",
    "    print(\"Global secondary index is being created, this may take a few minutes...\")\n",
    "    start = time.time()\n",
    "    while status == 'CREATING':\n",
    "        status = dt.check_status_gsi()\n",
    "        time.sleep(30)\n",
    "end = time.time()\n",
    "minute = (end - start) / 60\n",
    "print(\"Global secondary index created. Time elapsed: {0:.2f} minute\".format(minute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('workshop-data/data/logfile_medium1.csv', header=None)\n",
    "df_2 = pd.read_csv('workshop-data/data/logfile_medium2.csv', header=None)\n",
    "df_3 = pd.read_csv('workshop-data/data/logfile_stream.csv', header=None)\n",
    "df_4 = pd.read_csv('workshop-data/data/logfile_small1.csv', header=None)\n",
    "df = pd.concat([df_1, df_2, df_3, df_4])\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.columns = ['PK', 'host', 'date', 'hourofday', 'timezone', 'method', 'url', 'responsecode', 'bytessent', 'useragent']\n",
    "df['GSI_1_PK'] = df['host'].apply(lambda x: \"host#{}\".format(x))\n",
    "df['GSI_1_SK'] = df['responsecode'].astype(str) + \"#\" + df['date'] + \"#\" + df['hourofday'].astype(str)\n",
    "df.reset_index(inplace=True)\n",
    "df['PK'] = df['index']\n",
    "df.drop(labels=['index'], axis=1, inplace=True)\n",
    "df['PK'] = df['PK'].apply(lambda x: \"id#{}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data to DynamoDB table\n",
    "dt.batch_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from multiprocessing import Queue\n",
    "queue = Queue()\n",
    "\n",
    "scannedItems = 0\n",
    "totalbytessent = 0\n",
    "pageSize = 10000\n",
    "totalsegments = 2\n",
    "threadsegment = 1\n",
    "\n",
    "fe = \"responsecode <> :f\"\n",
    "eav = {\":f\": 200}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a parallel Scan request, TotalSegments represents the total number of segments into which the Scan operation will be divided. The value of TotalSegments corresponds to the number of application workers that will perform the parallel scan. For example, if you want to use four application threads to scan a table or an index, specify a TotalSegments value of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumed capacity:  119.0\n",
      "Total scanned items: 454\n"
     ]
    }
   ],
   "source": [
    "response = dt.table.scan(\n",
    "    FilterExpression=fe,\n",
    "    ExpressionAttributeValues=eav,\n",
    "    Limit=pageSize,\n",
    "    TotalSegments=totalsegments,\n",
    "    Segment=threadsegment,\n",
    "    ProjectionExpression='bytessent',\n",
    "    ReturnConsumedCapacity='TOTAL'\n",
    "    )\n",
    "scannedItems += len(response['Items'])\n",
    "for i in response['Items']:\n",
    "        totalbytessent += i['bytessent']\n",
    "print(\"Consumed capacity: \", response['ConsumedCapacity']['CapacityUnits'])\n",
    "\n",
    "while 'LastEvaluatedKey' in response:\n",
    "    key_value = response['LastEvaluatedKey']\n",
    "    response = dt.table.scan(\n",
    "        FilterExpression=fe,\n",
    "        ExpressionAttributeValues=eav,\n",
    "        Limit=pageSize,\n",
    "        TotalSegments=totalsegments,\n",
    "        Segment=threadsegment,\n",
    "        ExclusiveStartKey=key_value,\n",
    "        ProjectionExpression='bytessent'\n",
    "        )\n",
    "    scannedItems += len(response['Items'])\n",
    "    for i in response['Items']:\n",
    "        totalbytessent += i['bytessent']\n",
    "\n",
    "print(\"Total scanned items:\", scannedItems)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
